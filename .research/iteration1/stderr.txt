Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 138 packages in 125ms
Installed 104 packages in 3.53s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.2
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.2
 + certifi==2025.11.12
 + charset-normalizer==3.4.4
 + click==8.3.0
 + coloredlogs==15.0.1
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + filelock==3.20.0
 + flatbuffers==25.9.23
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + humanfriendly==10.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + joblib==1.5.2
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + onnxruntime-gpu==1.23.2
 + optuna==4.6.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.18.0
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.1
 + psutil==7.1.3
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.6.2
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.44.0
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + threadpoolctl==3.6.0
 + tokenizers==0.22.1
 + torch==2.9.1
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.1
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.23.0
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/pt80-1-a-29/_work/airas-20251115-064156-matsuzawa/airas-20251115-064156-matsuzawa/src/main.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/pt80-1-a-29/_work/airas-20251115-064156-matsuzawa/airas-20251115-064156-matsuzawa/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/pt80-1-a-29/_work/airas-20251115-064156-matsuzawa/airas-20251115-064156-matsuzawa/src/train.py:348: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/pt80-1-a-29/_work/airas-20251115-064156-matsuzawa/airas-20251115-064156-matsuzawa/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
`torch_dtype` is deprecated! Use `dtype` instead!
Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 220251.80 examples/s]
Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 208207.71 examples/s]
Map:   0%|          | 0/7099 [00:00<?, ? examples/s]Map:  20%|█▉        | 1407/7099 [00:00<00:00, 13973.17 examples/s]Map:  42%|████▏     | 3003/7099 [00:00<00:00, 15131.08 examples/s]Map:  66%|██████▌   | 4653/7099 [00:00<00:00, 15753.25 examples/s]Map:  89%|████████▊ | 6288/7099 [00:00<00:00, 15982.04 examples/s]Map: 100%|██████████| 7099/7099 [00:00<00:00, 14836.46 examples/s]
Map:   0%|          | 0/374 [00:00<?, ? examples/s]Map: 100%|██████████| 374/374 [00:00<00:00, 13063.86 examples/s]
Map:   0%|          | 0/7099 [00:00<?, ? examples/s]Map:  14%|█▍        | 1000/7099 [00:00<00:02, 2555.38 examples/s]Map:  28%|██▊       | 2000/7099 [00:00<00:01, 2660.87 examples/s]Map:  42%|████▏     | 3000/7099 [00:01<00:01, 2713.85 examples/s]Map:  56%|█████▋    | 4000/7099 [00:01<00:01, 2627.18 examples/s]Map:  70%|███████   | 5000/7099 [00:01<00:00, 2651.68 examples/s]Map:  85%|████████▍ | 6000/7099 [00:02<00:00, 2317.19 examples/s]Map:  99%|█████████▊| 7000/7099 [00:02<00:00, 2198.01 examples/s]Map: 100%|██████████| 7099/7099 [00:03<00:00, 2253.93 examples/s]
Map:   0%|          | 0/374 [00:00<?, ? examples/s]Map: 100%|██████████| 374/374 [00:00<00:00, 1834.76 examples/s]Map: 100%|██████████| 374/374 [00:00<00:00, 1735.21 examples/s]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 0/25 [00:00<?, ?it/s]Epoch 1 | loss=11.0058 | 4553.2 ms:   0%|          | 0/25 [00:04<?, ?it/s]Epoch 1 | loss=11.0058 | 4553.2 ms:   4%|▍         | 1/25 [00:04<01:49,  4.58s/it]Epoch 1 | loss=9.1615 | 186.0 ms:   4%|▍         | 1/25 [00:04<01:49,  4.58s/it]  Epoch 1 | loss=9.1615 | 186.0 ms:   8%|▊         | 2/25 [00:04<00:45,  2.00s/it]Epoch 1 | loss=8.8074 | 164.0 ms:   8%|▊         | 2/25 [00:04<00:45,  2.00s/it]Epoch 1 | loss=8.8074 | 164.0 ms:  12%|█▏        | 3/25 [00:04<00:25,  1.16s/it]Epoch 1 | loss=7.8778 | 161.2 ms:  12%|█▏        | 3/25 [00:05<00:25,  1.16s/it]Epoch 1 | loss=7.8778 | 161.2 ms:  16%|█▌        | 4/25 [00:05<00:16,  1.30it/s]Epoch 1 | loss=7.2555 | 161.4 ms:  16%|█▌        | 4/25 [00:05<00:16,  1.30it/s]Epoch 1 | loss=7.2555 | 161.4 ms:  20%|██        | 5/25 [00:05<00:11,  1.80it/s]Epoch 1 | loss=6.7933 | 161.8 ms:  20%|██        | 5/25 [00:05<00:11,  1.80it/s]Epoch 1 | loss=6.7933 | 161.8 ms:  24%|██▍       | 6/25 [00:05<00:08,  2.35it/s]Epoch 1 | loss=6.2652 | 161.4 ms:  24%|██▍       | 6/25 [00:05<00:08,  2.35it/s]Epoch 1 | loss=6.2652 | 161.4 ms:  28%|██▊       | 7/25 [00:05<00:06,  2.93it/s]Epoch 1 | loss=5.7379 | 161.4 ms:  28%|██▊       | 7/25 [00:05<00:06,  2.93it/s]Epoch 1 | loss=5.7379 | 161.4 ms:  32%|███▏      | 8/25 [00:05<00:04,  3.48it/s]Epoch 1 | loss=5.2067 | 162.3 ms:  32%|███▏      | 8/25 [00:05<00:04,  3.48it/s]Epoch 1 | loss=5.2067 | 162.3 ms:  36%|███▌      | 9/25 [00:05<00:04,  3.97it/s]Epoch 1 | loss=4.7441 | 161.3 ms:  36%|███▌      | 9/25 [00:06<00:04,  3.97it/s]Epoch 1 | loss=4.7441 | 161.3 ms:  40%|████      | 10/25 [00:06<00:03,  4.40it/s]Epoch 1 | loss=4.3513 | 170.7 ms:  40%|████      | 10/25 [00:06<00:03,  4.40it/s]Epoch 1 | loss=4.3513 | 170.7 ms:  44%|████▍     | 11/25 [00:06<00:02,  4.70it/s]Epoch 1 | loss=4.0335 | 164.4 ms:  44%|████▍     | 11/25 [00:06<00:02,  4.70it/s]Epoch 1 | loss=4.0335 | 164.4 ms:  48%|████▊     | 12/25 [00:06<00:02,  4.97it/s]Epoch 1 | loss=3.7664 | 165.0 ms:  48%|████▊     | 12/25 [00:06<00:02,  4.97it/s]Epoch 1 | loss=3.7664 | 165.0 ms:  52%|█████▏    | 13/25 [00:06<00:02,  5.18it/s]Epoch 1 | loss=3.5141 | 161.8 ms:  52%|█████▏    | 13/25 [00:06<00:02,  5.18it/s]Epoch 1 | loss=3.5141 | 161.8 ms:  56%|█████▌    | 14/25 [00:06<00:02,  5.36it/s]Epoch 1 | loss=3.3004 | 161.3 ms:  56%|█████▌    | 14/25 [00:07<00:02,  5.36it/s]Epoch 1 | loss=3.3004 | 161.3 ms:  60%|██████    | 15/25 [00:07<00:01,  5.50it/s]Epoch 1 | loss=3.1051 | 162.1 ms:  60%|██████    | 15/25 [00:07<00:01,  5.50it/s]Epoch 1 | loss=3.1051 | 162.1 ms:  64%|██████▍   | 16/25 [00:07<00:01,  5.59it/s]Epoch 1 | loss=2.9415 | 162.0 ms:  64%|██████▍   | 16/25 [00:07<00:01,  5.59it/s]Epoch 1 | loss=2.9415 | 162.0 ms:  68%|██████▊   | 17/25 [00:07<00:01,  5.66it/s]Epoch 1 | loss=2.7896 | 162.0 ms:  68%|██████▊   | 17/25 [00:07<00:01,  5.66it/s]Epoch 1 | loss=2.7896 | 162.0 ms:  72%|███████▏  | 18/25 [00:07<00:01,  5.71it/s]Epoch 1 | loss=2.6595 | 161.6 ms:  72%|███████▏  | 18/25 [00:07<00:01,  5.71it/s]Epoch 1 | loss=2.6595 | 161.6 ms:  76%|███████▌  | 19/25 [00:07<00:01,  5.74it/s]Epoch 1 | loss=2.5357 | 127.2 ms:  76%|███████▌  | 19/25 [00:07<00:01,  5.74it/s]Epoch 1 | loss=2.5357 | 127.2 ms:  80%|████████  | 20/25 [00:07<00:00,  6.13it/s]Epoch 1 | loss=2.4319 | 173.9 ms:  80%|████████  | 20/25 [00:08<00:00,  6.13it/s]Epoch 1 | loss=2.4319 | 173.9 ms:  84%|████████▍ | 21/25 [00:08<00:00,  5.98it/s]Epoch 1 | loss=2.3337 | 161.7 ms:  84%|████████▍ | 21/25 [00:08<00:00,  5.98it/s]Epoch 1 | loss=2.3337 | 161.7 ms:  88%|████████▊ | 22/25 [00:08<00:00,  5.91it/s]Epoch 1 | loss=2.2449 | 161.4 ms:  88%|████████▊ | 22/25 [00:08<00:00,  5.91it/s]Epoch 1 | loss=2.2449 | 161.4 ms:  92%|█████████▏| 23/25 [00:08<00:00,  5.89it/s]Epoch 1 | loss=2.1583 | 162.1 ms:  92%|█████████▏| 23/25 [00:08<00:00,  5.89it/s]Epoch 1 | loss=2.1583 | 162.1 ms:  96%|█████████▌| 24/25 [00:08<00:00,  5.87it/s]Epoch 1 | loss=2.0823 | 162.0 ms:  96%|█████████▌| 24/25 [00:08<00:00,  5.87it/s]Epoch 1 | loss=2.0823 | 162.0 ms: 100%|██████████| 25/25 [00:08<00:00,  5.86it/s]                                                                                   0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Val loss=0.0932:   0%|          | 0/25 [00:00<?, ?it/s]Val loss=0.0932:   4%|▍         | 1/25 [00:00<00:19,  1.23it/s]Val loss=0.2220:   4%|▍         | 1/25 [00:00<00:19,  1.23it/s]Val loss=0.2220:   8%|▊         | 2/25 [00:00<00:09,  2.52it/s]Val loss=0.2204:   8%|▊         | 2/25 [00:01<00:09,  2.52it/s]Val loss=0.2173:   8%|▊         | 2/25 [00:01<00:09,  2.52it/s]Val loss=0.2173:  16%|█▌        | 4/25 [00:01<00:04,  4.97it/s]Val loss=0.1949:  16%|█▌        | 4/25 [00:01<00:04,  4.97it/s]Val loss=0.1850:  16%|█▌        | 4/25 [00:01<00:04,  4.97it/s]Val loss=0.1850:  24%|██▍       | 6/25 [00:01<00:02,  6.82it/s]Val loss=0.1853:  24%|██▍       | 6/25 [00:01<00:02,  6.82it/s]Val loss=0.2026:  24%|██▍       | 6/25 [00:01<00:02,  6.82it/s]Val loss=0.2026:  32%|███▏      | 8/25 [00:01<00:02,  8.19it/s]Val loss=0.1953:  32%|███▏      | 8/25 [00:01<00:02,  8.19it/s]Val loss=0.2048:  32%|███▏      | 8/25 [00:01<00:02,  8.19it/s]Val loss=0.2048:  40%|████      | 10/25 [00:01<00:01, 10.22it/s]Val loss=0.2095:  40%|████      | 10/25 [00:01<00:01, 10.22it/s]Val loss=0.2106:  40%|████      | 10/25 [00:01<00:01, 10.22it/s]Val loss=0.2106:  48%|████▊     | 12/25 [00:01<00:01, 11.76it/s]Val loss=0.2064:  48%|████▊     | 12/25 [00:01<00:01, 11.76it/s]Val loss=0.2070:  48%|████▊     | 12/25 [00:01<00:01, 11.76it/s]Val loss=0.2070:  56%|█████▌    | 14/25 [00:01<00:00, 11.77it/s]Val loss=0.2101:  56%|█████▌    | 14/25 [00:01<00:00, 11.77it/s]Val loss=0.2057:  56%|█████▌    | 14/25 [00:02<00:00, 11.77it/s]Val loss=0.2057:  64%|██████▍   | 16/25 [00:02<00:00, 11.84it/s]Val loss=0.2052:  64%|██████▍   | 16/25 [00:02<00:00, 11.84it/s]Val loss=0.2023:  64%|██████▍   | 16/25 [00:02<00:00, 11.84it/s]Val loss=0.2023:  72%|███████▏  | 18/25 [00:02<00:00, 11.81it/s]Val loss=0.2055:  72%|███████▏  | 18/25 [00:02<00:00, 11.81it/s]Val loss=0.2009:  72%|███████▏  | 18/25 [00:02<00:00, 11.81it/s]Val loss=0.2009:  80%|████████  | 20/25 [00:02<00:00, 11.82it/s]Val loss=0.2026:  80%|████████  | 20/25 [00:02<00:00, 11.82it/s]Val loss=0.2001:  80%|████████  | 20/25 [00:02<00:00, 11.82it/s]Val loss=0.2001:  88%|████████▊ | 22/25 [00:02<00:00, 11.82it/s]Val loss=0.1977:  88%|████████▊ | 22/25 [00:02<00:00, 11.82it/s]Val loss=0.1981:  88%|████████▊ | 22/25 [00:02<00:00, 11.82it/s]Val loss=0.1981:  96%|█████████▌| 24/25 [00:02<00:00, 11.86it/s]Val loss=0.1976:  96%|█████████▌| 24/25 [00:02<00:00, 11.86it/s]                                                                  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Test loss=0.0932:   0%|          | 0/25 [00:00<?, ?it/s]Test loss=0.0932:   4%|▍         | 1/25 [00:00<00:07,  3.11it/s]Test loss=0.2220:   4%|▍         | 1/25 [00:00<00:07,  3.11it/s]Test loss=0.2204:   4%|▍         | 1/25 [00:00<00:07,  3.11it/s]Test loss=0.2204:  12%|█▏        | 3/25 [00:00<00:03,  6.84it/s]Test loss=0.2173:  12%|█▏        | 3/25 [00:00<00:03,  6.84it/s]Test loss=0.1949:  12%|█▏        | 3/25 [00:00<00:03,  6.84it/s]Test loss=0.1949:  20%|██        | 5/25 [00:00<00:02,  8.72it/s]Test loss=0.1850:  20%|██        | 5/25 [00:00<00:02,  8.72it/s]Test loss=0.1853:  20%|██        | 5/25 [00:00<00:02,  8.72it/s]Test loss=0.1853:  28%|██▊       | 7/25 [00:00<00:01,  9.76it/s]Test loss=0.2026:  28%|██▊       | 7/25 [00:00<00:01,  9.76it/s]Test loss=0.1953:  28%|██▊       | 7/25 [00:01<00:01,  9.76it/s]Test loss=0.1953:  36%|███▌      | 9/25 [00:01<00:01, 10.39it/s]Test loss=0.2048:  36%|███▌      | 9/25 [00:01<00:01, 10.39it/s]Test loss=0.2095:  36%|███▌      | 9/25 [00:01<00:01, 10.39it/s]Test loss=0.2095:  44%|████▍     | 11/25 [00:01<00:01, 10.90it/s]Test loss=0.2106:  44%|████▍     | 11/25 [00:01<00:01, 10.90it/s]Test loss=0.2064:  44%|████▍     | 11/25 [00:01<00:01, 10.90it/s]Test loss=0.2064:  52%|█████▏    | 13/25 [00:01<00:01, 11.78it/s]Test loss=0.2070:  52%|█████▏    | 13/25 [00:01<00:01, 11.78it/s]Test loss=0.2101:  52%|█████▏    | 13/25 [00:01<00:01, 11.78it/s]Test loss=0.2101:  60%|██████    | 15/25 [00:01<00:00, 12.76it/s]Test loss=0.2057:  60%|██████    | 15/25 [00:01<00:00, 12.76it/s]Test loss=0.2052:  60%|██████    | 15/25 [00:01<00:00, 12.76it/s]Test loss=0.2052:  68%|██████▊   | 17/25 [00:01<00:00, 12.44it/s]Test loss=0.2023:  68%|██████▊   | 17/25 [00:01<00:00, 12.44it/s]Test loss=0.2055:  68%|██████▊   | 17/25 [00:01<00:00, 12.44it/s]Test loss=0.2055:  76%|███████▌  | 19/25 [00:01<00:00, 12.23it/s]Test loss=0.2009:  76%|███████▌  | 19/25 [00:01<00:00, 12.23it/s]Test loss=0.2026:  76%|███████▌  | 19/25 [00:01<00:00, 12.23it/s]Test loss=0.2026:  84%|████████▍ | 21/25 [00:01<00:00, 12.09it/s]Test loss=0.2001:  84%|████████▍ | 21/25 [00:02<00:00, 12.09it/s]Test loss=0.1977:  84%|████████▍ | 21/25 [00:02<00:00, 12.09it/s]Test loss=0.1977:  92%|█████████▏| 23/25 [00:02<00:00, 12.00it/s]Test loss=0.1981:  92%|█████████▏| 23/25 [00:02<00:00, 12.00it/s]Test loss=0.1976:  92%|█████████▏| 23/25 [00:02<00:00, 12.00it/s]Test loss=0.1976: 100%|██████████| 25/25 [00:02<00:00, 11.93it/s]                                                                 